{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/sabrinasayed/Documents/GitHub/Fake-Job-Posts/Data/fake_job_postings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same Cleaning Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop location, job id, and salary range - they are either unnecessary or have too many missing values\n",
    "\n",
    "df.drop(columns=['job_id', 'location', 'salary_range'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title                  0\n",
       "department             0\n",
       "company_profile        0\n",
       "description            0\n",
       "requirements           0\n",
       "benefits               0\n",
       "telecommuting          0\n",
       "has_company_logo       0\n",
       "has_questions          0\n",
       "employment_type        0\n",
       "required_experience    0\n",
       "required_education     0\n",
       "industry               0\n",
       "function               0\n",
       "fraudulent             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace missing values in categorical columns with empty strings\n",
    "columns_with_text = ['title', 'department', 'company_profile', 'description', 'requirements',\n",
    "       'benefits', 'employment_type', 'required_experience',\n",
    "       'required_education', 'industry', 'function']\n",
    "df[columns_with_text] = df[columns_with_text].replace(np.nan, '')\n",
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/sabrinasayed/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sabrinasayed/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sabrinasayed/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sabrinasayed/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/sabrinasayed/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('omw-1.4') \n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Join tokens back into text\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>department</th>\n",
       "      <th>company_profile</th>\n",
       "      <th>description</th>\n",
       "      <th>requirements</th>\n",
       "      <th>benefits</th>\n",
       "      <th>telecommuting</th>\n",
       "      <th>has_company_logo</th>\n",
       "      <th>has_questions</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>required_experience</th>\n",
       "      <th>required_education</th>\n",
       "      <th>industry</th>\n",
       "      <th>function</th>\n",
       "      <th>fraudulent</th>\n",
       "      <th>title_processed</th>\n",
       "      <th>company_profile_processed</th>\n",
       "      <th>description_processed</th>\n",
       "      <th>requirements_processed</th>\n",
       "      <th>benefits_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Marketing Intern</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>We're Food52, and we've created a groundbreaki...</td>\n",
       "      <td>Food52, a fast-growing, James Beard Award-winn...</td>\n",
       "      <td>Experience with content management systems a m...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Other</td>\n",
       "      <td>Internship</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Marketing</td>\n",
       "      <td>0</td>\n",
       "      <td>marketing intern</td>\n",
       "      <td>food created groundbreaking award winning cook...</td>\n",
       "      <td>food fast growing james beard award winning on...</td>\n",
       "      <td>experience content management system major plu...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Customer Service - Cloud Video Production</td>\n",
       "      <td>Success</td>\n",
       "      <td>90 Seconds, the worlds Cloud Video Production ...</td>\n",
       "      <td>Organised - Focused - Vibrant - Awesome!Do you...</td>\n",
       "      <td>What we expect from you:Your key responsibilit...</td>\n",
       "      <td>What you will get from usThrough being part of...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Not Applicable</td>\n",
       "      <td></td>\n",
       "      <td>Marketing and Advertising</td>\n",
       "      <td>Customer Service</td>\n",
       "      <td>0</td>\n",
       "      <td>customer service cloud video production</td>\n",
       "      <td>second world cloud video production service se...</td>\n",
       "      <td>organised focused vibrant awesome passion cust...</td>\n",
       "      <td>expect key responsibility communicate client s...</td>\n",
       "      <td>get usthrough part second team gain experience...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Commissioning Machinery Assistant (CMA)</td>\n",
       "      <td></td>\n",
       "      <td>Valor Services provides Workforce Solutions th...</td>\n",
       "      <td>Our client, located in Houston, is actively se...</td>\n",
       "      <td>Implement pre-commissioning and commissioning ...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>commissioning machinery assistant cma</td>\n",
       "      <td>valor service provides workforce solution meet...</td>\n",
       "      <td>client located houston actively seeking experi...</td>\n",
       "      <td>implement pre commissioning commissioning proc...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Account Executive - Washington DC</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Our passion for improving quality of life thro...</td>\n",
       "      <td>THE COMPANY: ESRI – Environmental Systems Rese...</td>\n",
       "      <td>EDUCATION: Bachelor’s or Master’s in GIS, busi...</td>\n",
       "      <td>Our culture is anything but corporate—we have ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>Bachelor's Degree</td>\n",
       "      <td>Computer Software</td>\n",
       "      <td>Sales</td>\n",
       "      <td>0</td>\n",
       "      <td>account executive washington dc</td>\n",
       "      <td>passion improving quality life geography heart...</td>\n",
       "      <td>company esri environmental system research ins...</td>\n",
       "      <td>education bachelor master gi business administ...</td>\n",
       "      <td>culture anything corporate collaborative creat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bill Review Manager</td>\n",
       "      <td></td>\n",
       "      <td>SpotSource Solutions LLC is a Global Human Cap...</td>\n",
       "      <td>JOB TITLE: Itemization Review ManagerLOCATION:...</td>\n",
       "      <td>QUALIFICATIONS:RN license in the State of Texa...</td>\n",
       "      <td>Full Benefits Offered</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>Bachelor's Degree</td>\n",
       "      <td>Hospital &amp; Health Care</td>\n",
       "      <td>Health Care Provider</td>\n",
       "      <td>0</td>\n",
       "      <td>bill review manager</td>\n",
       "      <td>spotsource solution llc global human capital m...</td>\n",
       "      <td>job title itemization review managerlocation f...</td>\n",
       "      <td>qualification rn license state texasdiploma ba...</td>\n",
       "      <td>full benefit offered</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       title department  \\\n",
       "0                           Marketing Intern  Marketing   \n",
       "1  Customer Service - Cloud Video Production    Success   \n",
       "2    Commissioning Machinery Assistant (CMA)              \n",
       "3          Account Executive - Washington DC      Sales   \n",
       "4                        Bill Review Manager              \n",
       "\n",
       "                                     company_profile  \\\n",
       "0  We're Food52, and we've created a groundbreaki...   \n",
       "1  90 Seconds, the worlds Cloud Video Production ...   \n",
       "2  Valor Services provides Workforce Solutions th...   \n",
       "3  Our passion for improving quality of life thro...   \n",
       "4  SpotSource Solutions LLC is a Global Human Cap...   \n",
       "\n",
       "                                         description  \\\n",
       "0  Food52, a fast-growing, James Beard Award-winn...   \n",
       "1  Organised - Focused - Vibrant - Awesome!Do you...   \n",
       "2  Our client, located in Houston, is actively se...   \n",
       "3  THE COMPANY: ESRI – Environmental Systems Rese...   \n",
       "4  JOB TITLE: Itemization Review ManagerLOCATION:...   \n",
       "\n",
       "                                        requirements  \\\n",
       "0  Experience with content management systems a m...   \n",
       "1  What we expect from you:Your key responsibilit...   \n",
       "2  Implement pre-commissioning and commissioning ...   \n",
       "3  EDUCATION: Bachelor’s or Master’s in GIS, busi...   \n",
       "4  QUALIFICATIONS:RN license in the State of Texa...   \n",
       "\n",
       "                                            benefits  telecommuting  \\\n",
       "0                                                                 0   \n",
       "1  What you will get from usThrough being part of...              0   \n",
       "2                                                                 0   \n",
       "3  Our culture is anything but corporate—we have ...              0   \n",
       "4                              Full Benefits Offered              0   \n",
       "\n",
       "   has_company_logo  has_questions employment_type required_experience  \\\n",
       "0                 1              0           Other          Internship   \n",
       "1                 1              0       Full-time      Not Applicable   \n",
       "2                 1              0                                       \n",
       "3                 1              0       Full-time    Mid-Senior level   \n",
       "4                 1              1       Full-time    Mid-Senior level   \n",
       "\n",
       "  required_education                   industry              function  \\\n",
       "0                                                           Marketing   \n",
       "1                     Marketing and Advertising      Customer Service   \n",
       "2                                                                       \n",
       "3  Bachelor's Degree          Computer Software                 Sales   \n",
       "4  Bachelor's Degree     Hospital & Health Care  Health Care Provider   \n",
       "\n",
       "   fraudulent                          title_processed  \\\n",
       "0           0                         marketing intern   \n",
       "1           0  customer service cloud video production   \n",
       "2           0    commissioning machinery assistant cma   \n",
       "3           0          account executive washington dc   \n",
       "4           0                      bill review manager   \n",
       "\n",
       "                           company_profile_processed  \\\n",
       "0  food created groundbreaking award winning cook...   \n",
       "1  second world cloud video production service se...   \n",
       "2  valor service provides workforce solution meet...   \n",
       "3  passion improving quality life geography heart...   \n",
       "4  spotsource solution llc global human capital m...   \n",
       "\n",
       "                               description_processed  \\\n",
       "0  food fast growing james beard award winning on...   \n",
       "1  organised focused vibrant awesome passion cust...   \n",
       "2  client located houston actively seeking experi...   \n",
       "3  company esri environmental system research ins...   \n",
       "4  job title itemization review managerlocation f...   \n",
       "\n",
       "                              requirements_processed  \\\n",
       "0  experience content management system major plu...   \n",
       "1  expect key responsibility communicate client s...   \n",
       "2  implement pre commissioning commissioning proc...   \n",
       "3  education bachelor master gi business administ...   \n",
       "4  qualification rn license state texasdiploma ba...   \n",
       "\n",
       "                                  benefits_processed  \n",
       "0                                                     \n",
       "1  get usthrough part second team gain experience...  \n",
       "2                                                     \n",
       "3  culture anything corporate collaborative creat...  \n",
       "4                               full benefit offered  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Apply preprocessing to relevant text columns\n",
    "text_columns = ['title', 'company_profile', 'description', 'requirements', 'benefits']\n",
    "for column in text_columns:\n",
    "    df[f'{column}_processed'] = df[column].apply(preprocess_text)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking title_processed:\n",
      "Remaining special characters:\n",
      "[('_', 1)]\n",
      "\n",
      "Checking company_profile_processed:\n",
      "Remaining special characters:\n",
      "[('_', 3827)]\n",
      "\n",
      "Checking description_processed:\n",
      "Remaining special characters:\n",
      "[('_', 8945)]\n",
      "\n",
      "Checking requirements_processed:\n",
      "Remaining special characters:\n",
      "[('_', 2901)]\n",
      "\n",
      "Checking benefits_processed:\n",
      "Remaining special characters:\n",
      "[('_', 3683)]\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Normalize unicode characters\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    \n",
    "    # Remove non-ASCII characters\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Join tokens back into text\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Reapply the improved preprocessing\n",
    "text_columns = ['title', 'company_profile', 'description', 'requirements', 'benefits']\n",
    "for column in text_columns:\n",
    "    df[f'{column}_processed'] = df[column].apply(preprocess_text)\n",
    "\n",
    "# Verify the cleaning worked\n",
    "def check_cleaning(text_series):\n",
    "    all_text = ' '.join(text_series)\n",
    "    special_chars = re.findall(r'[^a-zA-Z\\s]', all_text)\n",
    "    remaining = Counter(special_chars).most_common()\n",
    "    \n",
    "    if remaining:\n",
    "        print(\"Remaining special characters:\")\n",
    "        print(remaining)\n",
    "    else:\n",
    "        print(\"No special characters remaining!\")\n",
    "\n",
    "# Check the results\n",
    "for column in text_columns:\n",
    "    print(f\"\\nChecking {column}_processed:\")\n",
    "    check_cleaning(df[f'{column}_processed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ability</th>\n",
       "      <th>ability build</th>\n",
       "      <th>ability communicate</th>\n",
       "      <th>ability effectively</th>\n",
       "      <th>ability learn</th>\n",
       "      <th>ability manage</th>\n",
       "      <th>ability multi</th>\n",
       "      <th>ability multi task</th>\n",
       "      <th>ability prioritize</th>\n",
       "      <th>ability work</th>\n",
       "      <th>...</th>\n",
       "      <th>youll</th>\n",
       "      <th>youll need</th>\n",
       "      <th>youll work</th>\n",
       "      <th>young</th>\n",
       "      <th>youre</th>\n",
       "      <th>youre looking</th>\n",
       "      <th>youve</th>\n",
       "      <th>yr</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.011413</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.019763</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4248 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ability  ability build  ability communicate  ability effectively  \\\n",
       "0  0.000000            0.0                  0.0                  0.0   \n",
       "1  0.011413            0.0                  0.0                  0.0   \n",
       "2  0.000000            0.0                  0.0                  0.0   \n",
       "3  0.019763            0.0                  0.0                  0.0   \n",
       "4  0.000000            0.0                  0.0                  0.0   \n",
       "\n",
       "   ability learn  ability manage  ability multi  ability multi task  \\\n",
       "0            0.0             0.0            0.0                 0.0   \n",
       "1            0.0             0.0            0.0                 0.0   \n",
       "2            0.0             0.0            0.0                 0.0   \n",
       "3            0.0             0.0            0.0                 0.0   \n",
       "4            0.0             0.0            0.0                 0.0   \n",
       "\n",
       "   ability prioritize  ability work  ...  youll  youll need  youll work  \\\n",
       "0                 0.0           0.0  ...    0.0         0.0         0.0   \n",
       "1                 0.0           0.0  ...    0.0         0.0         0.0   \n",
       "2                 0.0           0.0  ...    0.0         0.0         0.0   \n",
       "3                 0.0           0.0  ...    0.0         0.0         0.0   \n",
       "4                 0.0           0.0  ...    0.0         0.0         0.0   \n",
       "\n",
       "   young  youre  youre looking  youve   yr  zealand  zone  \n",
       "0    0.0    0.0            0.0    0.0  0.0      0.0   0.0  \n",
       "1    0.0    0.0            0.0    0.0  0.0      0.0   0.0  \n",
       "2    0.0    0.0            0.0    0.0  0.0      0.0   0.0  \n",
       "3    0.0    0.0            0.0    0.0  0.0      0.0   0.0  \n",
       "4    0.0    0.0            0.0    0.0  0.0      0.0   0.0  \n",
       "\n",
       "[5 rows x 4248 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Combine relevant text columns for vectorization\n",
    "df['combined_text']= (df['title_processed'] + ' ' + \n",
    "                      df['company_profile_processed'] + ' ' + \n",
    "                      df['description_processed'] + ' ' + \n",
    "                      df['requirements_processed'] + ' ' + \n",
    "                      df['benefits_processed'])\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df = 0.01, \n",
    "                             max_df = 0.95, \n",
    "                             ngram_range=(1,3), \n",
    "                             stop_words='english')\n",
    "\n",
    "X_tfidf = vectorizer.fit_transform(df['combined_text'])\n",
    "\n",
    "vec_tfidf = pd.DataFrame(X_tfidf.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "vec_tfidf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: company, finance, financial, service, benefit, employment, position, year, well, credit\n",
      "Topic 2: marketing, medium, digital, social, content, brand, campaign, online, experience, new\n",
      "Topic 3: business, management, client, project, team, experience, skill, work, service, company\n",
      "Topic 4: experience, job, technical, system, engineering, year, website, amp, data, manufacturing\n",
      "Topic 5: experience, product, design, development, software, technology, service, application, team, user\n",
      "Topic 6: student, job, teacher, abroad, get, loan, teaching, required, experience, title\n",
      "Topic 7: customer, service, work, process, business, document, communication, solution, required, mail\n",
      "Topic 8: sale, customer, product, service, business, work, career, role, candidate, training\n",
      "Topic 9: service, work, home, time, care, must, experience, position, customer, hour\n",
      "Topic 10: team, work, experience, people, new, working, company, looking, product, want\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Document-term matrix using CountVectorizer\n",
    "count_vectorizer = CountVectorizer(min_df=0.01, max_df=0.95)\n",
    "doc_term_matrix = count_vectorizer.fit_transform(df['combined_text'])\n",
    "\n",
    "# FLDA model\n",
    "# n_components represents the number of topics you want to extract\n",
    "n_topics = 10\n",
    "lda_model = LatentDirichletAllocation(n_components=n_topics, \n",
    "                                     random_state=42,\n",
    "                                     learning_method='batch')\n",
    "lda_output = lda_model.fit_transform(doc_term_matrix)\n",
    "\n",
    "# Display the top words for each topic\n",
    "def print_topics(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words = [feature_names[i] for i in topic.argsort()[:-n_top_words-1:-1]]\n",
    "        print(f\"Topic {topic_idx + 1}: {', '.join(top_words)}\")\n",
    "\n",
    "# Display top 10 words for each topic\n",
    "print_topics(lda_model, count_vectorizer.get_feature_names_out(), 10)\n",
    "\n",
    "# Add topic distributions to original dataframe\n",
    "topic_names = [f\"Topic {i+1}\" for i in range(n_topics)]\n",
    "doc_topics = pd.DataFrame(lda_output, columns=topic_names)\n",
    "\n",
    "# Get dominant topic for each document\n",
    "df['dominant_topic'] = doc_topics.idxmax(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Advanced Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new sophisticated NLP features\n",
    "def extract_advanced_text_features(text):\n",
    "    if not isinstance(text, str) or len(text.strip()) == 0:\n",
    "        return {\n",
    "            # Basic features (from original)\n",
    "            'avg_word_length': 0,\n",
    "            'caps_ratio': 0,\n",
    "            'url_count': 0,\n",
    "            'email_pattern': 0,\n",
    "            'money_pattern': 0,\n",
    "            # New advanced features\n",
    "            'num_sentences': 0,\n",
    "            'avg_sentence_length': 0,\n",
    "            'num_entities': 0,\n",
    "            'sentiment_score': 0\n",
    "        }\n",
    "    \n",
    "    nlp = spacy.load('en_core_web_md')\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Basic features (keep original calculations)\n",
    "    words = text.split()\n",
    "    avg_word_length = np.mean([len(word) for word in words]) if words else 0\n",
    "    text_length = len(text)\n",
    "    caps_ratio = sum(1 for c in text if c.isupper()) / text_length if text_length > 0 else 0\n",
    "    \n",
    "    # New advanced features\n",
    "    sentences = list(doc.sents)\n",
    "    sentiment = TextBlob(text).sentiment\n",
    "    \n",
    "    return {\n",
    "        # Original features\n",
    "        'avg_word_length': avg_word_length,\n",
    "        'caps_ratio': caps_ratio,\n",
    "        'url_count': text.count('http'),\n",
    "        'email_pattern': len(re.findall(r'[\\w\\.-]+@[\\w\\.-]+', text)),\n",
    "        'money_pattern': len(re.findall(r'[\\$£€]\\d+', text)),\n",
    "        # New features\n",
    "        'num_sentences': len(sentences),\n",
    "        'avg_sentence_length': np.mean([len(str(sent).split()) for sent in sentences]) if sentences else 0,\n",
    "        'num_entities': len(doc.ents),\n",
    "        'sentiment_score': sentiment.polarity\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bigram/trigram features\n",
    "def extract_ngram_features(text_series, min_df=5):\n",
    "    ngram_vectorizer = CountVectorizer(\n",
    "        ngram_range=(2, 3),\n",
    "        min_df=min_df,\n",
    "        stop_words='english'\n",
    "    )\n",
    "    \n",
    "    ngrams = ngram_vectorizer.fit_transform(text_series)\n",
    "    return pd.DataFrame(\n",
    "        ngrams.toarray(),\n",
    "        columns=[f'ngram_{feat}' for feat in ngram_vectorizer.get_feature_names_out()]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing title with advanced features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sabrinasayed/Documents/Flatiron/Phase 5/.conda/lib/python3.11/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_md' (3.8.0) was trained with spaCy v3.8.0 and may not be 100% compatible with the current version (3.7.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with advanced features...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Extract advanced features\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m advanced_features \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_processed\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(extract_advanced_text_features)\n\u001b[1;32m      7\u001b[0m advanced_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(advanced_features\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m      8\u001b[0m advanced_df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m advanced_df\u001b[38;5;241m.\u001b[39mcolumns]\n",
      "File \u001b[0;32m~/Documents/Flatiron/Phase 5/.conda/lib/python3.11/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[1;32m   4918\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4919\u001b[0m         func,\n\u001b[1;32m   4920\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[1;32m   4921\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[1;32m   4922\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   4923\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m-> 4924\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[0;32m~/Documents/Flatiron/Phase 5/.conda/lib/python3.11/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[0;32m~/Documents/Flatiron/Phase 5/.conda/lib/python3.11/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[1;32m   1508\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[1;32m   1509\u001b[0m )\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/Documents/Flatiron/Phase 5/.conda/lib/python3.11/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[0;32m~/Documents/Flatiron/Phase 5/.conda/lib/python3.11/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[12], line 18\u001b[0m, in \u001b[0;36mextract_advanced_text_features\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text\u001b[38;5;241m.\u001b[39mstrip()) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;66;03m# Basic features (from original)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_word_length\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment_score\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     16\u001b[0m     }\n\u001b[0;32m---> 18\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men_core_web_md\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(text)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Basic features (keep original calculations)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Flatiron/Phase 5/.conda/lib/python3.11/site-packages/spacy/__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[1;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m util\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[1;32m     52\u001b[0m         name,\n\u001b[1;32m     53\u001b[0m         vocab\u001b[38;5;241m=\u001b[39mvocab,\n\u001b[1;32m     54\u001b[0m         disable\u001b[38;5;241m=\u001b[39mdisable,\n\u001b[1;32m     55\u001b[0m         enable\u001b[38;5;241m=\u001b[39menable,\n\u001b[1;32m     56\u001b[0m         exclude\u001b[38;5;241m=\u001b[39mexclude,\n\u001b[1;32m     57\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m     58\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/Flatiron/Phase 5/.conda/lib/python3.11/site-packages/spacy/util.py:465\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_lang_class(name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblank:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))()\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_package(name):  \u001b[38;5;66;03m# installed as package\u001b[39;00m\n\u001b[0;32m--> 465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m load_model_from_package(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Path(name)\u001b[38;5;241m.\u001b[39mexists():  \u001b[38;5;66;03m# path to model data directory\u001b[39;00m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m load_model_from_path(Path(name), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Flatiron/Phase 5/.conda/lib/python3.11/site-packages/spacy/util.py:501\u001b[0m, in \u001b[0;36mload_model_from_package\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load a model from an installed package.\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \n\u001b[1;32m    486\u001b[0m \u001b[38;5;124;03mname (str): The package name.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;124;03mRETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(name)\n\u001b[0;32m--> 501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mload(vocab\u001b[38;5;241m=\u001b[39mvocab, disable\u001b[38;5;241m=\u001b[39mdisable, enable\u001b[38;5;241m=\u001b[39menable, exclude\u001b[38;5;241m=\u001b[39mexclude, config\u001b[38;5;241m=\u001b[39mconfig)\n",
      "File \u001b[0;32m~/Documents/Flatiron/Phase 5/.conda/lib/python3.11/site-packages/en_core_web_md/__init__.py:10\u001b[0m, in \u001b[0;36mload\u001b[0;34m(**overrides)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moverrides):\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m load_model_from_init_py(\u001b[38;5;18m__file__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moverrides)\n",
      "File \u001b[0;32m~/Documents/Flatiron/Phase 5/.conda/lib/python3.11/site-packages/spacy/util.py:682\u001b[0m, in \u001b[0;36mload_model_from_init_py\u001b[0;34m(init_file, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m    681\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE052\u001b[38;5;241m.\u001b[39mformat(path\u001b[38;5;241m=\u001b[39mdata_path))\n\u001b[0;32m--> 682\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m load_model_from_path(\n\u001b[1;32m    683\u001b[0m     data_path,\n\u001b[1;32m    684\u001b[0m     vocab\u001b[38;5;241m=\u001b[39mvocab,\n\u001b[1;32m    685\u001b[0m     meta\u001b[38;5;241m=\u001b[39mmeta,\n\u001b[1;32m    686\u001b[0m     disable\u001b[38;5;241m=\u001b[39mdisable,\n\u001b[1;32m    687\u001b[0m     enable\u001b[38;5;241m=\u001b[39menable,\n\u001b[1;32m    688\u001b[0m     exclude\u001b[38;5;241m=\u001b[39mexclude,\n\u001b[1;32m    689\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m    690\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Flatiron/Phase 5/.conda/lib/python3.11/site-packages/spacy/util.py:547\u001b[0m, in \u001b[0;36mload_model_from_path\u001b[0;34m(model_path, meta, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    538\u001b[0m config \u001b[38;5;241m=\u001b[39m load_config(config_path, overrides\u001b[38;5;241m=\u001b[39moverrides)\n\u001b[1;32m    539\u001b[0m nlp \u001b[38;5;241m=\u001b[39m load_model_from_config(\n\u001b[1;32m    540\u001b[0m     config,\n\u001b[1;32m    541\u001b[0m     vocab\u001b[38;5;241m=\u001b[39mvocab,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    545\u001b[0m     meta\u001b[38;5;241m=\u001b[39mmeta,\n\u001b[1;32m    546\u001b[0m )\n\u001b[0;32m--> 547\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nlp\u001b[38;5;241m.\u001b[39mfrom_disk(model_path, exclude\u001b[38;5;241m=\u001b[39mexclude, overrides\u001b[38;5;241m=\u001b[39moverrides)\n",
      "File \u001b[0;32m~/Documents/Flatiron/Phase 5/.conda/lib/python3.11/site-packages/spacy/language.py:2184\u001b[0m, in \u001b[0;36mfrom_disk\u001b[0;34m(self, path, exclude, overrides)\u001b[0m\n\u001b[1;32m   2182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(enable, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   2183\u001b[0m     enable \u001b[38;5;241m=\u001b[39m [enable]\n\u001b[0;32m-> 2184\u001b[0m to_disable \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   2185\u001b[0m     \u001b[38;5;241m*\u001b[39m[pipe_name \u001b[38;5;28;01mfor\u001b[39;00m pipe_name \u001b[38;5;129;01min\u001b[39;00m pipe_names \u001b[38;5;28;01mif\u001b[39;00m pipe_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m enable],\n\u001b[1;32m   2186\u001b[0m     \u001b[38;5;241m*\u001b[39mdisable,\n\u001b[1;32m   2187\u001b[0m }\n\u001b[1;32m   2188\u001b[0m \u001b[38;5;66;03m# If any pipe to be enabled is in to_disable, the specification is inconsistent.\u001b[39;00m\n\u001b[1;32m   2189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(enable) \u001b[38;5;241m&\u001b[39m to_disable):\n",
      "File \u001b[0;32m~/Documents/Flatiron/Phase 5/.conda/lib/python3.11/site-packages/spacy/util.py:1372\u001b[0m, in \u001b[0;36mfrom_disk\u001b[0;34m(path, readers, exclude)\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_disk\u001b[39m(\n\u001b[1;32m   1367\u001b[0m     path: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m   1368\u001b[0m     writers: Dict[\u001b[38;5;28mstr\u001b[39m, Callable[[Path], \u001b[38;5;28;01mNone\u001b[39;00m]],\n\u001b[1;32m   1369\u001b[0m     exclude: Iterable[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   1370\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Path:\n\u001b[1;32m   1371\u001b[0m     path \u001b[38;5;241m=\u001b[39m ensure_path(path)\n\u001b[0;32m-> 1372\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m   1373\u001b[0m         path\u001b[38;5;241m.\u001b[39mmkdir()\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, writer \u001b[38;5;129;01min\u001b[39;00m writers\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   1375\u001b[0m         \u001b[38;5;66;03m# Split to support file names like meta.json\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Flatiron/Phase 5/.conda/lib/python3.11/site-packages/spacy/language.py:2178\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(p, proc)\u001b[0m\n\u001b[1;32m   2165\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Derives whether (1) `disable` and `enable` values are consistent and (2)\u001b[39;00m\n\u001b[1;32m   2166\u001b[0m \u001b[38;5;124;03mresolves those to a single set of disabled components. Raises an error in\u001b[39;00m\n\u001b[1;32m   2167\u001b[0m \u001b[38;5;124;03mcase of inconsistency.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2174\u001b[0m \u001b[38;5;124;03m                           specified includes and excludes.\u001b[39;00m\n\u001b[1;32m   2175\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(disable, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m-> 2178\u001b[0m     disable \u001b[38;5;241m=\u001b[39m [disable]\n\u001b[1;32m   2179\u001b[0m to_disable \u001b[38;5;241m=\u001b[39m disable\n\u001b[1;32m   2181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m enable:\n",
      "File \u001b[0;32m~/Documents/Flatiron/Phase 5/.conda/lib/python3.11/site-packages/spacy/pipeline/trainable_pipe.pyx:343\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.from_disk\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Documents/Flatiron/Phase 5/.conda/lib/python3.11/site-packages/spacy/util.py:1372\u001b[0m, in \u001b[0;36mfrom_disk\u001b[0;34m(path, readers, exclude)\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_disk\u001b[39m(\n\u001b[1;32m   1367\u001b[0m     path: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m   1368\u001b[0m     writers: Dict[\u001b[38;5;28mstr\u001b[39m, Callable[[Path], \u001b[38;5;28;01mNone\u001b[39;00m]],\n\u001b[1;32m   1369\u001b[0m     exclude: Iterable[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   1370\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Path:\n\u001b[1;32m   1371\u001b[0m     path \u001b[38;5;241m=\u001b[39m ensure_path(path)\n\u001b[0;32m-> 1372\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m   1373\u001b[0m         path\u001b[38;5;241m.\u001b[39mmkdir()\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, writer \u001b[38;5;129;01min\u001b[39;00m writers\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   1375\u001b[0m         \u001b[38;5;66;03m# Split to support file names like meta.json\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Flatiron/Phase 5/.conda/lib/python3.11/site-packages/spacy/pipeline/trainable_pipe.pyx:333\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.from_disk.load_model\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Documents/Flatiron/Phase 5/.conda/lib/python3.11/site-packages/spacy/pipeline/trainable_pipe.pyx:334\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.from_disk.load_model\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Documents/Flatiron/Phase 5/.conda/lib/python3.11/site-packages/thinc/model.py:638\u001b[0m, in \u001b[0;36mModel.from_bytes\u001b[0;34m(self, bytes_data)\u001b[0m\n\u001b[1;32m    636\u001b[0m msg \u001b[38;5;241m=\u001b[39m srsly\u001b[38;5;241m.\u001b[39mmsgpack_loads(bytes_data)\n\u001b[1;32m    637\u001b[0m msg \u001b[38;5;241m=\u001b[39m convert_recursive(is_xp_array, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39masarray, msg)\n\u001b[0;32m--> 638\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_dict(msg)\n",
      "File \u001b[0;32m~/Documents/Flatiron/Phase 5/.conda/lib/python3.11/site-packages/thinc/model.py:669\u001b[0m, in \u001b[0;36mModel.from_dict\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attr, value \u001b[38;5;129;01min\u001b[39;00m msg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattrs\u001b[39m\u001b[38;5;124m\"\u001b[39m][i]\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    668\u001b[0m     default_value \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mattrs\u001b[38;5;241m.\u001b[39mget(attr)\n\u001b[0;32m--> 669\u001b[0m     loaded_value \u001b[38;5;241m=\u001b[39m deserialize_attr(default_value, value, attr, node)\n\u001b[1;32m    670\u001b[0m     node\u001b[38;5;241m.\u001b[39mattrs[attr] \u001b[38;5;241m=\u001b[39m loaded_value\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param_name, value \u001b[38;5;129;01min\u001b[39;00m msg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m][i]\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/Documents/Flatiron/Phase 5/.conda/lib/python3.11/functools.py:909\u001b[0m, in \u001b[0;36msingledispatch.<locals>.wrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires at least \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    907\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1 positional argument\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 909\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dispatch(args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[0;32m~/Documents/Flatiron/Phase 5/.conda/lib/python3.11/site-packages/thinc/model.py:849\u001b[0m, in \u001b[0;36mdeserialize_attr\u001b[0;34m(_, value, name, model)\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39msingledispatch\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeserialize_attr\u001b[39m(_: Any, value: Any, name: \u001b[38;5;28mstr\u001b[39m, model: Model) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    845\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize an attribute value (defaults to msgpack). You can register\u001b[39;00m\n\u001b[1;32m    846\u001b[0m \u001b[38;5;124;03m    custom deserializers using the @deserialize_attr.register decorator with the\u001b[39;00m\n\u001b[1;32m    847\u001b[0m \u001b[38;5;124;03m    type to deserialize, e.g.: @deserialize_attr.register(MyCustomObject).\u001b[39;00m\n\u001b[1;32m    848\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 849\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m srsly\u001b[38;5;241m.\u001b[39mmsgpack_loads(value)\n",
      "File \u001b[0;32m~/Documents/Flatiron/Phase 5/.conda/lib/python3.11/site-packages/srsly/_msgpack_api.py:27\u001b[0m, in \u001b[0;36mmsgpack_loads\u001b[0;34m(data, use_list)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# msgpack-python docs suggest disabling gc before unpacking large messages\u001b[39;00m\n\u001b[1;32m     26\u001b[0m gc\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[0;32m---> 27\u001b[0m msg \u001b[38;5;241m=\u001b[39m msgpack\u001b[38;5;241m.\u001b[39mloads(data, raw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, use_list\u001b[38;5;241m=\u001b[39muse_list)\n\u001b[1;32m     28\u001b[0m gc\u001b[38;5;241m.\u001b[39menable()\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m msg\n",
      "File \u001b[0;32m~/Documents/Flatiron/Phase 5/.conda/lib/python3.11/site-packages/srsly/msgpack/__init__.py:76\u001b[0m, in \u001b[0;36munpackb\u001b[0;34m(packed, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject_pairs_hook\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m     75\u001b[0m     object_hook \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject_hook\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m decoder \u001b[38;5;129;01min\u001b[39;00m msgpack_decoders\u001b[38;5;241m.\u001b[39mget_all()\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m     77\u001b[0m         object_hook \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(decoder, chain\u001b[38;5;241m=\u001b[39mobject_hook)\n\u001b[1;32m     78\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject_hook\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m object_hook\n",
      "File \u001b[0;32m~/Documents/Flatiron/Phase 5/.conda/lib/python3.11/site-packages/catalogue/__init__.py:113\u001b[0m, in \u001b[0;36mRegistry.get_all\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m     result\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_entry_points())\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m keys, value \u001b[38;5;129;01min\u001b[39;00m REGISTRY\u001b[38;5;241m.\u001b[39mcopy()\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamespace) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(keys) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[0;32m--> 113\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamespace[i] \u001b[38;5;241m==\u001b[39m keys[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamespace))\n\u001b[1;32m    114\u001b[0m     ):\n\u001b[1;32m    115\u001b[0m         result[keys[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Apply advanced features to each text column\n",
    "for column in text_columns:\n",
    "    print(f\"\\nProcessing {column} with advanced features...\")\n",
    "    \n",
    "    # Extract advanced features\n",
    "    advanced_features = df[f'{column}_processed'].apply(extract_advanced_text_features)\n",
    "    advanced_df = pd.DataFrame(advanced_features.tolist())\n",
    "    advanced_df.columns = [f'{column}_{col}' for col in advanced_df.columns]\n",
    "    \n",
    "    # Extract n-gram features\n",
    "    ngram_df = extract_ngram_features(df[f'{column}_processed'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put it all together\n",
    "\n",
    "cleaned_df = pd.concat([df, advanced_df, ngram_df, vec_tfidf], axis=1)\n",
    "cleaned_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
