{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/sabrinasayed/Documents/GitHub/Fake-Job-Posts/Data/fake_job_postings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same Cleaning Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop location, job id, and salary range - they are either unnecessary or have too many missing values\n",
    "\n",
    "df.drop(columns=['job_id', 'location', 'salary_range'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title                  0\n",
       "department             0\n",
       "company_profile        0\n",
       "description            0\n",
       "requirements           0\n",
       "benefits               0\n",
       "telecommuting          0\n",
       "has_company_logo       0\n",
       "has_questions          0\n",
       "employment_type        0\n",
       "required_experience    0\n",
       "required_education     0\n",
       "industry               0\n",
       "function               0\n",
       "fraudulent             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace missing values in categorical columns with empty strings\n",
    "columns_with_text = ['title', 'department', 'company_profile', 'description', 'requirements',\n",
    "       'benefits', 'employment_type', 'required_experience',\n",
    "       'required_education', 'industry', 'function']\n",
    "df[columns_with_text] = df[columns_with_text].replace(np.nan, '')\n",
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/sabrinasayed/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sabrinasayed/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sabrinasayed/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sabrinasayed/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/sabrinasayed/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('omw-1.4') \n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Join tokens back into text\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>department</th>\n",
       "      <th>company_profile</th>\n",
       "      <th>description</th>\n",
       "      <th>requirements</th>\n",
       "      <th>benefits</th>\n",
       "      <th>telecommuting</th>\n",
       "      <th>has_company_logo</th>\n",
       "      <th>has_questions</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>required_experience</th>\n",
       "      <th>required_education</th>\n",
       "      <th>industry</th>\n",
       "      <th>function</th>\n",
       "      <th>fraudulent</th>\n",
       "      <th>title_processed</th>\n",
       "      <th>company_profile_processed</th>\n",
       "      <th>description_processed</th>\n",
       "      <th>requirements_processed</th>\n",
       "      <th>benefits_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Marketing Intern</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>We're Food52, and we've created a groundbreaki...</td>\n",
       "      <td>Food52, a fast-growing, James Beard Award-winn...</td>\n",
       "      <td>Experience with content management systems a m...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Other</td>\n",
       "      <td>Internship</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Marketing</td>\n",
       "      <td>0</td>\n",
       "      <td>marketing intern</td>\n",
       "      <td>food created groundbreaking award winning cook...</td>\n",
       "      <td>food fast growing james beard award winning on...</td>\n",
       "      <td>experience content management system major plu...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Customer Service - Cloud Video Production</td>\n",
       "      <td>Success</td>\n",
       "      <td>90 Seconds, the worlds Cloud Video Production ...</td>\n",
       "      <td>Organised - Focused - Vibrant - Awesome!Do you...</td>\n",
       "      <td>What we expect from you:Your key responsibilit...</td>\n",
       "      <td>What you will get from usThrough being part of...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Not Applicable</td>\n",
       "      <td></td>\n",
       "      <td>Marketing and Advertising</td>\n",
       "      <td>Customer Service</td>\n",
       "      <td>0</td>\n",
       "      <td>customer service cloud video production</td>\n",
       "      <td>second world cloud video production service se...</td>\n",
       "      <td>organised focused vibrant awesome passion cust...</td>\n",
       "      <td>expect key responsibility communicate client s...</td>\n",
       "      <td>get usthrough part second team gain experience...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Commissioning Machinery Assistant (CMA)</td>\n",
       "      <td></td>\n",
       "      <td>Valor Services provides Workforce Solutions th...</td>\n",
       "      <td>Our client, located in Houston, is actively se...</td>\n",
       "      <td>Implement pre-commissioning and commissioning ...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>commissioning machinery assistant cma</td>\n",
       "      <td>valor service provides workforce solution meet...</td>\n",
       "      <td>client located houston actively seeking experi...</td>\n",
       "      <td>implement pre commissioning commissioning proc...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Account Executive - Washington DC</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Our passion for improving quality of life thro...</td>\n",
       "      <td>THE COMPANY: ESRI – Environmental Systems Rese...</td>\n",
       "      <td>EDUCATION: Bachelor’s or Master’s in GIS, busi...</td>\n",
       "      <td>Our culture is anything but corporate—we have ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>Bachelor's Degree</td>\n",
       "      <td>Computer Software</td>\n",
       "      <td>Sales</td>\n",
       "      <td>0</td>\n",
       "      <td>account executive washington dc</td>\n",
       "      <td>passion improving quality life geography heart...</td>\n",
       "      <td>company esri environmental system research ins...</td>\n",
       "      <td>education bachelor master gi business administ...</td>\n",
       "      <td>culture anything corporate collaborative creat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bill Review Manager</td>\n",
       "      <td></td>\n",
       "      <td>SpotSource Solutions LLC is a Global Human Cap...</td>\n",
       "      <td>JOB TITLE: Itemization Review ManagerLOCATION:...</td>\n",
       "      <td>QUALIFICATIONS:RN license in the State of Texa...</td>\n",
       "      <td>Full Benefits Offered</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>Bachelor's Degree</td>\n",
       "      <td>Hospital &amp; Health Care</td>\n",
       "      <td>Health Care Provider</td>\n",
       "      <td>0</td>\n",
       "      <td>bill review manager</td>\n",
       "      <td>spotsource solution llc global human capital m...</td>\n",
       "      <td>job title itemization review managerlocation f...</td>\n",
       "      <td>qualification rn license state texasdiploma ba...</td>\n",
       "      <td>full benefit offered</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       title department  \\\n",
       "0                           Marketing Intern  Marketing   \n",
       "1  Customer Service - Cloud Video Production    Success   \n",
       "2    Commissioning Machinery Assistant (CMA)              \n",
       "3          Account Executive - Washington DC      Sales   \n",
       "4                        Bill Review Manager              \n",
       "\n",
       "                                     company_profile  \\\n",
       "0  We're Food52, and we've created a groundbreaki...   \n",
       "1  90 Seconds, the worlds Cloud Video Production ...   \n",
       "2  Valor Services provides Workforce Solutions th...   \n",
       "3  Our passion for improving quality of life thro...   \n",
       "4  SpotSource Solutions LLC is a Global Human Cap...   \n",
       "\n",
       "                                         description  \\\n",
       "0  Food52, a fast-growing, James Beard Award-winn...   \n",
       "1  Organised - Focused - Vibrant - Awesome!Do you...   \n",
       "2  Our client, located in Houston, is actively se...   \n",
       "3  THE COMPANY: ESRI – Environmental Systems Rese...   \n",
       "4  JOB TITLE: Itemization Review ManagerLOCATION:...   \n",
       "\n",
       "                                        requirements  \\\n",
       "0  Experience with content management systems a m...   \n",
       "1  What we expect from you:Your key responsibilit...   \n",
       "2  Implement pre-commissioning and commissioning ...   \n",
       "3  EDUCATION: Bachelor’s or Master’s in GIS, busi...   \n",
       "4  QUALIFICATIONS:RN license in the State of Texa...   \n",
       "\n",
       "                                            benefits  telecommuting  \\\n",
       "0                                                                 0   \n",
       "1  What you will get from usThrough being part of...              0   \n",
       "2                                                                 0   \n",
       "3  Our culture is anything but corporate—we have ...              0   \n",
       "4                              Full Benefits Offered              0   \n",
       "\n",
       "   has_company_logo  has_questions employment_type required_experience  \\\n",
       "0                 1              0           Other          Internship   \n",
       "1                 1              0       Full-time      Not Applicable   \n",
       "2                 1              0                                       \n",
       "3                 1              0       Full-time    Mid-Senior level   \n",
       "4                 1              1       Full-time    Mid-Senior level   \n",
       "\n",
       "  required_education                   industry              function  \\\n",
       "0                                                           Marketing   \n",
       "1                     Marketing and Advertising      Customer Service   \n",
       "2                                                                       \n",
       "3  Bachelor's Degree          Computer Software                 Sales   \n",
       "4  Bachelor's Degree     Hospital & Health Care  Health Care Provider   \n",
       "\n",
       "   fraudulent                          title_processed  \\\n",
       "0           0                         marketing intern   \n",
       "1           0  customer service cloud video production   \n",
       "2           0    commissioning machinery assistant cma   \n",
       "3           0          account executive washington dc   \n",
       "4           0                      bill review manager   \n",
       "\n",
       "                           company_profile_processed  \\\n",
       "0  food created groundbreaking award winning cook...   \n",
       "1  second world cloud video production service se...   \n",
       "2  valor service provides workforce solution meet...   \n",
       "3  passion improving quality life geography heart...   \n",
       "4  spotsource solution llc global human capital m...   \n",
       "\n",
       "                               description_processed  \\\n",
       "0  food fast growing james beard award winning on...   \n",
       "1  organised focused vibrant awesome passion cust...   \n",
       "2  client located houston actively seeking experi...   \n",
       "3  company esri environmental system research ins...   \n",
       "4  job title itemization review managerlocation f...   \n",
       "\n",
       "                              requirements_processed  \\\n",
       "0  experience content management system major plu...   \n",
       "1  expect key responsibility communicate client s...   \n",
       "2  implement pre commissioning commissioning proc...   \n",
       "3  education bachelor master gi business administ...   \n",
       "4  qualification rn license state texasdiploma ba...   \n",
       "\n",
       "                                  benefits_processed  \n",
       "0                                                     \n",
       "1  get usthrough part second team gain experience...  \n",
       "2                                                     \n",
       "3  culture anything corporate collaborative creat...  \n",
       "4                               full benefit offered  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Apply preprocessing to relevant text columns\n",
    "text_columns = ['title', 'company_profile', 'description', 'requirements', 'benefits']\n",
    "for column in text_columns:\n",
    "    df[f'{column}_processed'] = df[column].apply(preprocess_text)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking title_processed:\n",
      "Remaining special characters:\n",
      "[('_', 1)]\n",
      "\n",
      "Checking company_profile_processed:\n",
      "Remaining special characters:\n",
      "[('_', 3827)]\n",
      "\n",
      "Checking description_processed:\n",
      "Remaining special characters:\n",
      "[('_', 8945)]\n",
      "\n",
      "Checking requirements_processed:\n",
      "Remaining special characters:\n",
      "[('_', 2901)]\n",
      "\n",
      "Checking benefits_processed:\n",
      "Remaining special characters:\n",
      "[('_', 3683)]\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Normalize unicode characters\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    \n",
    "    # Remove non-ASCII characters\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Join tokens back into text\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Reapply the improved preprocessing\n",
    "text_columns = ['title', 'company_profile', 'description', 'requirements', 'benefits']\n",
    "for column in text_columns:\n",
    "    df[f'{column}_processed'] = df[column].apply(preprocess_text)\n",
    "\n",
    "# Verify the cleaning worked\n",
    "def check_cleaning(text_series):\n",
    "    all_text = ' '.join(text_series)\n",
    "    special_chars = re.findall(r'[^a-zA-Z\\s]', all_text)\n",
    "    remaining = Counter(special_chars).most_common()\n",
    "    \n",
    "    if remaining:\n",
    "        print(\"Remaining special characters:\")\n",
    "        print(remaining)\n",
    "    else:\n",
    "        print(\"No special characters remaining!\")\n",
    "\n",
    "# Check the results\n",
    "for column in text_columns:\n",
    "    print(f\"\\nChecking {column}_processed:\")\n",
    "    check_cleaning(df[f'{column}_processed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ability</th>\n",
       "      <th>ability build</th>\n",
       "      <th>ability communicate</th>\n",
       "      <th>ability effectively</th>\n",
       "      <th>ability learn</th>\n",
       "      <th>ability manage</th>\n",
       "      <th>ability multi</th>\n",
       "      <th>ability multi task</th>\n",
       "      <th>ability prioritize</th>\n",
       "      <th>ability work</th>\n",
       "      <th>...</th>\n",
       "      <th>youll</th>\n",
       "      <th>youll need</th>\n",
       "      <th>youll work</th>\n",
       "      <th>young</th>\n",
       "      <th>youre</th>\n",
       "      <th>youre looking</th>\n",
       "      <th>youve</th>\n",
       "      <th>yr</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.011413</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.019763</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4248 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ability  ability build  ability communicate  ability effectively  \\\n",
       "0  0.000000            0.0                  0.0                  0.0   \n",
       "1  0.011413            0.0                  0.0                  0.0   \n",
       "2  0.000000            0.0                  0.0                  0.0   \n",
       "3  0.019763            0.0                  0.0                  0.0   \n",
       "4  0.000000            0.0                  0.0                  0.0   \n",
       "\n",
       "   ability learn  ability manage  ability multi  ability multi task  \\\n",
       "0            0.0             0.0            0.0                 0.0   \n",
       "1            0.0             0.0            0.0                 0.0   \n",
       "2            0.0             0.0            0.0                 0.0   \n",
       "3            0.0             0.0            0.0                 0.0   \n",
       "4            0.0             0.0            0.0                 0.0   \n",
       "\n",
       "   ability prioritize  ability work  ...  youll  youll need  youll work  \\\n",
       "0                 0.0           0.0  ...    0.0         0.0         0.0   \n",
       "1                 0.0           0.0  ...    0.0         0.0         0.0   \n",
       "2                 0.0           0.0  ...    0.0         0.0         0.0   \n",
       "3                 0.0           0.0  ...    0.0         0.0         0.0   \n",
       "4                 0.0           0.0  ...    0.0         0.0         0.0   \n",
       "\n",
       "   young  youre  youre looking  youve   yr  zealand  zone  \n",
       "0    0.0    0.0            0.0    0.0  0.0      0.0   0.0  \n",
       "1    0.0    0.0            0.0    0.0  0.0      0.0   0.0  \n",
       "2    0.0    0.0            0.0    0.0  0.0      0.0   0.0  \n",
       "3    0.0    0.0            0.0    0.0  0.0      0.0   0.0  \n",
       "4    0.0    0.0            0.0    0.0  0.0      0.0   0.0  \n",
       "\n",
       "[5 rows x 4248 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Combine relevant text columns for vectorization\n",
    "df['combined_text']= (df['title_processed'] + ' ' + \n",
    "                      df['company_profile_processed'] + ' ' + \n",
    "                      df['description_processed'] + ' ' + \n",
    "                      df['requirements_processed'] + ' ' + \n",
    "                      df['benefits_processed'])\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df = 0.01, \n",
    "                             max_df = 0.95, \n",
    "                             ngram_range=(1,3), \n",
    "                             stop_words='english')\n",
    "\n",
    "X_tfidf = vectorizer.fit_transform(df['combined_text'])\n",
    "\n",
    "vec_tfidf = pd.DataFrame(X_tfidf.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "vec_tfidf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: company, finance, financial, service, benefit, employment, position, year, well, credit\n",
      "Topic 2: marketing, medium, digital, social, content, brand, campaign, online, experience, new\n",
      "Topic 3: business, management, client, project, team, experience, skill, work, service, company\n",
      "Topic 4: experience, job, technical, system, engineering, year, website, amp, data, manufacturing\n",
      "Topic 5: experience, product, design, development, software, technology, service, application, team, user\n",
      "Topic 6: student, job, teacher, abroad, get, loan, teaching, required, experience, title\n",
      "Topic 7: customer, service, work, process, business, document, communication, solution, required, mail\n",
      "Topic 8: sale, customer, product, service, business, work, career, role, candidate, training\n",
      "Topic 9: service, work, home, time, care, must, experience, position, customer, hour\n",
      "Topic 10: team, work, experience, people, new, working, company, looking, product, want\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Document-term matrix using CountVectorizer\n",
    "count_vectorizer = CountVectorizer(min_df=0.01, max_df=0.95)\n",
    "doc_term_matrix = count_vectorizer.fit_transform(df['combined_text'])\n",
    "\n",
    "# FLDA model\n",
    "# n_components represents the number of topics you want to extract\n",
    "n_topics = 10\n",
    "lda_model = LatentDirichletAllocation(n_components=n_topics, \n",
    "                                     random_state=42,\n",
    "                                     learning_method='batch')\n",
    "lda_output = lda_model.fit_transform(doc_term_matrix)\n",
    "\n",
    "# Display the top words for each topic\n",
    "def print_topics(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words = [feature_names[i] for i in topic.argsort()[:-n_top_words-1:-1]]\n",
    "        print(f\"Topic {topic_idx + 1}: {', '.join(top_words)}\")\n",
    "\n",
    "# Display top 10 words for each topic\n",
    "print_topics(lda_model, count_vectorizer.get_feature_names_out(), 10)\n",
    "\n",
    "# Add topic distributions to original dataframe\n",
    "topic_names = [f\"Topic {i+1}\" for i in range(n_topics)]\n",
    "doc_topics = pd.DataFrame(lda_output, columns=topic_names)\n",
    "\n",
    "# Get dominant topic for each document\n",
    "df['dominant_topic'] = doc_topics.idxmax(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Advanced Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new sophisticated NLP features\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "def extract_advanced_text_features(text, nlp_model):\n",
    "    if not isinstance(text, str) or len(text.strip()) == 0:\n",
    "        return {\n",
    "            'avg_word_length': 0,\n",
    "            'caps_ratio': 0,\n",
    "            'url_count': 0,\n",
    "            'email_pattern': 0,\n",
    "            'money_pattern': 0,\n",
    "            'num_sentences': 0,\n",
    "            'avg_sentence_length': 0,\n",
    "            'num_entities': 0,\n",
    "            'sentiment_score': 0\n",
    "        }\n",
    "    \n",
    "    doc = nlp_model(text)\n",
    "\n",
    "    # Basic features (keep original calculations)\n",
    "    words = text.split()\n",
    "    avg_word_length = np.mean([len(word) for word in words]) if words else 0\n",
    "    text_length = len(text)\n",
    "    caps_ratio = sum(1 for c in text if c.isupper()) / text_length if text_length > 0 else 0\n",
    "    \n",
    "    # New advanced features\n",
    "    sentences = list(doc.sents)\n",
    "    sentiment = TextBlob(text).sentiment\n",
    "    \n",
    "    return {\n",
    "        # Original features\n",
    "        'avg_word_length': avg_word_length,\n",
    "        'caps_ratio': caps_ratio,\n",
    "        'url_count': text.count('http'),\n",
    "        'email_pattern': len(re.findall(r'[\\w\\.-]+@[\\w\\.-]+', text)),\n",
    "        'money_pattern': len(re.findall(r'[\\$£€]\\d+', text)),\n",
    "        # New features\n",
    "        'num_sentences': len(sentences),\n",
    "        'avg_sentence_length': np.mean([len(str(sent).split()) for sent in sentences]) if sentences else 0,\n",
    "        'num_entities': len(doc.ents),\n",
    "        'sentiment_score': sentiment.polarity\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bigram/trigram features\n",
    "def extract_ngram_features(text_series, min_df=5):\n",
    "    ngram_vectorizer = CountVectorizer(\n",
    "        ngram_range=(2, 3),\n",
    "        min_df=min_df,\n",
    "        stop_words='english'\n",
    "    )\n",
    "    \n",
    "    ngrams = ngram_vectorizer.fit_transform(text_series)\n",
    "    return pd.DataFrame(\n",
    "        ngrams.toarray(),\n",
    "        columns=[f'ngram_{feat}' for feat in ngram_vectorizer.get_feature_names_out()]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing all columns...\n",
      "\n",
      "Processing title...\n",
      "\n",
      "Processing company_profile...\n",
      "\n",
      "Processing description...\n",
      "\n",
      "Processing requirements...\n",
      "\n",
      "Processing benefits...\n"
     ]
    }
   ],
   "source": [
    "# Apply advanced features to each text column\n",
    "# Process all columns at once\n",
    "print(\"Processing all columns...\")\n",
    "\n",
    "# Combine all text for n-gram features\n",
    "all_text = df[[f'{column}_processed' for column in text_columns]].agg(' '.join, axis=1)\n",
    "ngram_df = extract_ngram_features(all_text)\n",
    "\n",
    "# Process advanced features for each column\n",
    "all_advanced_features = {}\n",
    "for column in text_columns:\n",
    "    print(f\"\\nProcessing {column}...\")\n",
    "    advanced_features = df[f'{column}_processed'].apply(lambda x: extract_advanced_text_features(x, nlp))\n",
    "    advanced_df = pd.DataFrame(advanced_features.tolist())\n",
    "    all_advanced_features.update({f'{column}_{col}': advanced_df[col] for col in advanced_df.columns}) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put it all together\n",
    "\n",
    "advanced_features_df = pd.DataFrame(all_advanced_features)\n",
    "cleaned_df = pd.concat([df, advanced_features_df, ngram_df, vec_tfidf], axis=1)\n",
    "cleaned_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
